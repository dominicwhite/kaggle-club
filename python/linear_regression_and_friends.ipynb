{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "linear_regression_and_friends.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPynVQyNICLxliDJAEpuHea",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dominicwhite/kaggle-club/blob/master/python/linear_regression_and_friends.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1HGcgjG4JIq",
        "colab_type": "text"
      },
      "source": [
        "# Linear regression and friends"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLQjTSeb6GMG",
        "colab_type": "text"
      },
      "source": [
        "## The `diabetes` dataset\n",
        "\n",
        "We will be using sklearn's built-in diabetes dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybCVY5Et5Hok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import datasets\n",
        "\n",
        "diabetes = datasets.load_diabetes()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4_IQTbB597P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(diabetes.DESCR)\n",
        "print(diabetes.feature_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djrZICdh5Yli",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "diabetes_X = diabetes.data\n",
        "diabetes_y = diabetes.target\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(diabetes_X, diabetes_y, test_size = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DFqO4bo4e4a",
        "colab_type": "text"
      },
      "source": [
        "## Basic Linear regression in `scikit-learn`\n",
        "\n",
        "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/438px-Linear_regression.svg.png)\n",
        "\n",
        "For linear regression, we can use the `LinearRegression` class.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRM6UBwJ36Dd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "\n",
        "lin_reg.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XR3uMwxc7nAX",
        "colab_type": "text"
      },
      "source": [
        "We can now look at the linear model we just created.\n",
        "\n",
        "The coefficients are stored in the `coef_` attribute.\n",
        "\n",
        "> **Pro-tip**: any attributed created by the `fit(...)` method will end with an underscore."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKd-UuMm7jsk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(lin_reg.coef_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnaH7f_O89_n",
        "colab_type": "text"
      },
      "source": [
        "Let's make this a bit more readable by associating each coefficient with its variable name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbNqtgMp8eOk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(lin_reg.coef_, index = diabetes.feature_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCktJOe-80S1",
        "colab_type": "text"
      },
      "source": [
        "What about the intercept?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTvtgSCb82Tg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Intercept: \\n', lin_reg.intercept_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYkqI7CS9fXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lin_reg_y_pred = lin_reg.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIl0x7M298KM",
        "colab_type": "text"
      },
      "source": [
        "We can calculate some error metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2C41xdgE9rYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "print('Mean squared error: %.2f'\n",
        "      % mean_squared_error(y_test, lin_reg_y_pred))\n",
        "# The R-squared value (aka coefficient of determination): 1 is perfect prediction\n",
        "print('R-squared: %.2f'\n",
        "      % r2_score(y_test, lin_reg_y_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_jrQHxi-B8G",
        "colab_type": "text"
      },
      "source": [
        "## How does linear regression work?\n",
        "\n",
        "We find the line that minimizes the sum of the squared residuals.\n",
        "\n",
        "This is also known as the mean squared error (MSE).\n",
        "\n",
        "It turns out that the MSE is always a parabola w.r.t. the coefficients of the model:\n",
        "\n",
        "![alt text](https://www.onlinemath4all.com/images/minimumvalueofparabola.png)\n",
        "\n",
        "For linear regression we can find an exact solution.\n",
        "\n",
        "\n",
        "\n",
        "*   Differentiate the cost function equation (MSE)\n",
        "*   Set the differential == 0.\n",
        "*   Solve!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9gqRGyyM_CL",
        "colab_type": "text"
      },
      "source": [
        "## Polynomial regression aka. how to get lots of features\n",
        "\n",
        "A standard linear regression might look like this:\n",
        "\n",
        "$y = m_1 x_1 + m_2 x_2 + c$\n",
        "\n",
        "But this assumes that y and each x are linearly related.\n",
        "\n",
        "Maybe they aren't in which case we can transform the variables, e.g. by taking their logarithm, or by calculating polynomial variations:\n",
        "\n",
        "$y = m_1 x_1 + m_2 x_2 + m_3 x_1^2 + m_4 x_2^2 + c$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRZV9x8cOL0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "poly_features = PolynomialFeatures(degree=2, include_bias = False)\n",
        "X_train_poly = poly_features.fit_transform(X_train)\n",
        "\n",
        "lin_reg_poly = LinearRegression()\n",
        "lin_reg_poly.fit(X_train_poly, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQSB8sNUOpyp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_poly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eL_olN1X_-Ja",
        "colab_type": "text"
      },
      "source": [
        "## Regularized Linear Models\n",
        "\n",
        "If we have a lot of features in our training data, we are likely to overfit.\n",
        "\n",
        "However, we don't want to arbitrarily exclude features - after all they might include useful information.\n",
        "\n",
        "One solution is **regularization**: we penalize features in our cost function as well as the fit.\n",
        "\n",
        "For example, in standard linear regression:\n",
        "\n",
        "cost_function = MSE\n",
        "\n",
        "In regularized linear regression:\n",
        "\n",
        "cost_function = MSE + regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uPHNTEA-6Ud",
        "colab_type": "text"
      },
      "source": [
        "Alternatively we can find an approximation of the minimum using gradient descent:\n",
        "\n",
        "![alt text](https://images.deepai.org/glossary-terms/dd6cdd6fcfea4af1a1075aac0b5aa110/sgd.png)\n",
        "\n",
        "This can be a lot quicker when you have large amounts of data (lots of rows), or a lot of features in X (lots of columns).\n",
        "\n",
        "However, you have to decide on a learning rate (don't want this to be too big or too small)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psno_nnI_2HX",
        "colab_type": "text"
      },
      "source": [
        "### Ridge regression\n",
        "\n",
        "In ridge regression, the regularization term is the sum of the coefficients squared.\n",
        "\n",
        "So, our cost function is:\n",
        "\n",
        "$cost function = MSE + \\alpha \\sum{(coefs)^2}$\n",
        "\n",
        "where \n",
        "* $\\alpha$ is a constant set by **you** that determines how strong this regularization penalty is.\n",
        "\n",
        "There's a great video on Youtube that introduces Ridge regression very simply and shows how it's predictions compare to standard regression (~20 mins).\n",
        "\n",
        "> *Note: the narrator recommends a bunch of other videos to watch first, but you can ignore most of these. The Bias-Variance video might be useful if you have never heard of those concepts, but you can also just think of \"Bias\" as underfitting, and \"Variance\" as overfitting.*\n",
        "\n",
        "<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=Q81RR3yKn30\n",
        "\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/Q81RR3yKn30/0.jpg\" \n",
        "alt=\"Elastic Net\" width=\"480\" height=\"360\" border=\"1\" /></a>\n",
        "\n",
        "In Scikit-Learn, we can use the `Ridge` class to do Ridge regression just like we did standard linear regression (more details on Ridge regression with sklearn [in the official guide](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvGP3TvdCtlD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "ridge_reg = Ridge(alpha = 1) # note that we have to set alpha\n",
        "\n",
        "ridge_reg.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBOYU_EzDLpK",
        "colab_type": "text"
      },
      "source": [
        "#### Exercises\n",
        "\n",
        "1. Repeat the steps we carried out for the linear regression above to look at the models fit and accuracy.\n",
        "\n",
        "2. What happens if you set alpha to 0?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pD3abip2DnXX",
        "colab_type": "text"
      },
      "source": [
        "### Lasso regression\n",
        "\n",
        "Like ridge regression, but our regularization term is sum of the absolute values of the coefficients (rather than their squares).\n",
        "\n",
        "$cost function = MSE + \\alpha \\sum{|coefs|}$\n",
        "\n",
        "The same Youtube channel as before has another (even shorter) video on Lasso regression:\n",
        "\n",
        "<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=NGf0voTMlcs\n",
        "\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/NGf0voTMlcs/0.jpg\" \n",
        "alt=\"Elastic Net\" width=\"480\" height=\"360\" border=\"1\" /></a>\n",
        "\n",
        "In Scikit-Learn, we can use the `Lasso` class to do Lasso regression just like we did standard or Ridge regression (more details on Lasso regression with sklearn [in the official guide](https://scikit-learn.org/stable/modules/linear_model.html#lasso))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXsZpxc9Dmua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "lasso_reg = Lasso(alpha = 1) # note that we have to set alpha\n",
        "\n",
        "lasso_reg.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEE_UTYXFT4j",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise\n",
        "\n",
        "1. Repeat the steps we carried out for the linear regression above to look at the models fit and accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0Tik7nWFVvv",
        "colab_type": "text"
      },
      "source": [
        "### More exercises\n",
        "\n",
        "1. We can combine Ridge and Lasso regression into the same regression model. This is called **ElasticNet** and we can use a sklearn class of the same name: `ElasticNet` (more details in the [documentation](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression)). Try using Elastic Net to create an a different regression model.\n",
        "\n",
        "  You can learn more about Elastic Net in this Youtube video:\n",
        "\n",
        "  <a href=\"http://www.youtube.com/watch?feature=player_embedded&v=1dKRdX9bfIo\n",
        "\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/1dKRdX9bfIo/0.jpg\" \n",
        "alt=\"Elastic Net\" width=\"480\" height=\"360\" border=\"1\" /></a>\n",
        "\n",
        "2. We have to set alpha (the model can't figure it out for us). How do we decide the best value? **Cross-validation!**\n",
        "\n",
        "  i. One way of getting the optimal value of alpha with cross validation is to manually search through values of alpha using the `GridSearchCV` class. Link to documentation \n",
        "\n",
        "  ii. Alternatively there are versions of each regularized linear regression which will do cross validation much more simply:\n",
        "    * [LassoCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html)\n",
        "    * [RidgeCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html)\n",
        "    * [ElasticNetCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNetCV.html)\n",
        "\n",
        "    For example, you can use code like this:\n",
        "\n",
        "    ```python\n",
        "    from sklearn.linear_model import ElasticNetCV\n",
        "\n",
        "    # Values of alpha to try:\n",
        "    alphas = [0.1, 0.5, 1, 2, 4, 8]\n",
        "\n",
        "    elastic_cv = ElasticNetCV(alphas=alphas, cv=5)\n",
        "    elastic_cv.fit(X_train, y_train)\n",
        "    \n",
        "    # Then we use elastic_cv like any of the previous models:\n",
        "    print(elastic_cv.alpha_)\n",
        "    print(elastic_cv.intercept_)\n",
        "    ```"
      ]
    }
  ]
}