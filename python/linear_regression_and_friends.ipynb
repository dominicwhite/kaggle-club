{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "linear_regression_and_friends.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPynVQyNICLxliDJAEpuHea",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dominicwhite/kaggle-club/blob/master/python/linear_regression_and_friends.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1HGcgjG4JIq",
        "colab_type": "text"
      },
      "source": [
        "# Linear regression and friends"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLQjTSeb6GMG",
        "colab_type": "text"
      },
      "source": [
        "## The `diabetes` dataset\n",
        "\n",
        "We will be using sklearn's built-in diabetes dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybCVY5Et5Hok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import datasets\n",
        "\n",
        "diabetes = datasets.load_diabetes()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4_IQTbB597P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "outputId": "dc57afc9-44df-41c8-a465-ac57e1e56e12"
      },
      "source": [
        "print(diabetes.DESCR)\n",
        "print(diabetes.feature_names)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".. _diabetes_dataset:\n",
            "\n",
            "Diabetes dataset\n",
            "----------------\n",
            "\n",
            "Ten baseline variables, age, sex, body mass index, average blood\n",
            "pressure, and six blood serum measurements were obtained for each of n =\n",
            "442 diabetes patients, as well as the response of interest, a\n",
            "quantitative measure of disease progression one year after baseline.\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "  :Number of Instances: 442\n",
            "\n",
            "  :Number of Attributes: First 10 columns are numeric predictive values\n",
            "\n",
            "  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n",
            "\n",
            "  :Attribute Information:\n",
            "      - Age\n",
            "      - Sex\n",
            "      - Body mass index\n",
            "      - Average blood pressure\n",
            "      - S1\n",
            "      - S2\n",
            "      - S3\n",
            "      - S4\n",
            "      - S5\n",
            "      - S6\n",
            "\n",
            "Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\n",
            "\n",
            "Source URL:\n",
            "https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n",
            "\n",
            "For more information see:\n",
            "Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n",
            "(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n",
            "['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djrZICdh5Yli",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "diabetes_X = diabetes.data\n",
        "diabetes_y = diabetes.target\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(diabetes_X, diabetes_y, test_size = 0.2)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DFqO4bo4e4a",
        "colab_type": "text"
      },
      "source": [
        "## Basic Linear regression in `scikit-learn`\n",
        "\n",
        "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/438px-Linear_regression.svg.png)\n",
        "\n",
        "For linear regression, we can use the `LinearRegression` class.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRM6UBwJ36Dd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "af43c5e2-d049-49be-8e73-54ac6f7a5e30"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "\n",
        "lin_reg.fit(X_train, y_train)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XR3uMwxc7nAX",
        "colab_type": "text"
      },
      "source": [
        "We can now look at the linear model we just created.\n",
        "\n",
        "The coefficients are stored in the `coef_` attribute.\n",
        "\n",
        "> **Pro-tip**: any attributed created by the `fit(...)` method will end with an underscore."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKd-UuMm7jsk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "224d0a8f-0433-4650-9cac-df0fe6eca883"
      },
      "source": [
        "print(lin_reg.coef_)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   8.99632085 -250.8480026   563.13634469  294.92140304 -904.9661982\n",
            "  470.10412496  162.68727107  268.50280092  809.68561994   36.01795276]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnaH7f_O89_n",
        "colab_type": "text"
      },
      "source": [
        "Let's make this a bit more readable by associating each coefficient with its variable name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbNqtgMp8eOk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "1ae71949-b705-4323-bd9f-c7059caa19e0"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(lin_reg.coef_, index = diabetes.feature_names)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>age</th>\n",
              "      <td>8.996321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sex</th>\n",
              "      <td>-250.848003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bmi</th>\n",
              "      <td>563.136345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bp</th>\n",
              "      <td>294.921403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>s1</th>\n",
              "      <td>-904.966198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>s2</th>\n",
              "      <td>470.104125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>s3</th>\n",
              "      <td>162.687271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>s4</th>\n",
              "      <td>268.502801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>s5</th>\n",
              "      <td>809.685620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>s6</th>\n",
              "      <td>36.017953</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              0\n",
              "age    8.996321\n",
              "sex -250.848003\n",
              "bmi  563.136345\n",
              "bp   294.921403\n",
              "s1  -904.966198\n",
              "s2   470.104125\n",
              "s3   162.687271\n",
              "s4   268.502801\n",
              "s5   809.685620\n",
              "s6    36.017953"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCktJOe-80S1",
        "colab_type": "text"
      },
      "source": [
        "What about the intercept?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTvtgSCb82Tg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c4774764-374c-4770-b9cc-4d168155a828"
      },
      "source": [
        "print('Intercept: \\n', lin_reg.intercept_)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Intercept: \n",
            " 150.77434173919147\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYkqI7CS9fXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lin_reg_y_pred = lin_reg.predict(X_test)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIl0x7M298KM",
        "colab_type": "text"
      },
      "source": [
        "We can calculate some error metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2C41xdgE9rYA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "967df0b8-40db-49f5-fd16-ee4b833dde57"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "print('Mean squared error: %.2f'\n",
        "      % mean_squared_error(y_test, lin_reg_y_pred))\n",
        "# The R-squared value (aka coefficient of determination): 1 is perfect prediction\n",
        "print('R-squared: %.2f'\n",
        "      % r2_score(y_test, lin_reg_y_pred))\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean squared error: 2994.83\n",
            "R-squared: 0.46\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_jrQHxi-B8G",
        "colab_type": "text"
      },
      "source": [
        "## How does linear regression work?\n",
        "\n",
        "We find the line that minimizes the sum of the squared residuals.\n",
        "\n",
        "This is also known as the mean squared error (MSE).\n",
        "\n",
        "It turns out that the MSE is always a parabola w.r.t. the coefficients of the model:\n",
        "\n",
        "![alt text](https://www.onlinemath4all.com/images/minimumvalueofparabola.png)\n",
        "\n",
        "For linear regression we can find an exact solution.\n",
        "\n",
        "\n",
        "\n",
        "*   Differentiate the cost function equation (MSE)\n",
        "*   Set the differential == 0.\n",
        "*   Solve!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9gqRGyyM_CL",
        "colab_type": "text"
      },
      "source": [
        "## Polynomial regression aka. how to get lots of features\n",
        "\n",
        "A standard linear regression might look like this:\n",
        "\n",
        "$y = m_1 x_1 + m_2 x_2 + c$\n",
        "\n",
        "But this assumes that y and each x are linearly related.\n",
        "\n",
        "Maybe they aren't in which case we can transform the variables, e.g. by taking their logarithm, or by calculating polynomial variations:\n",
        "\n",
        "$y = m_1 x_1 + m_2 x_2 + m_3 x_1^2 + m_4 x_2^2 + c$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRZV9x8cOL0D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e90915ca-b34d-42b6-e209-81c8b2bc5287"
      },
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "poly_features = PolynomialFeatures(degree=2, include_bias = False)\n",
        "X_train_poly = poly_features.fit_transform(X_train)\n",
        "\n",
        "lin_reg_poly = LinearRegression()\n",
        "lin_reg_poly.fit(X_train_poly, y_train)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQSB8sNUOpyp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "6eb3d08f-d8fe-4d8c-95c6-a0076a5936c9"
      },
      "source": [
        "X_train_poly"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.64121703e-02, -4.46416365e-02, -1.05172024e-02, ...,\n",
              "         4.57689584e-04,  7.31975227e-04,  1.17063562e-03],\n",
              "       [ 1.26481373e-02, -4.46416365e-02, -2.56065715e-02, ...,\n",
              "         5.20251396e-03, -8.18558656e-04,  1.28791250e-04],\n",
              "       [-1.88201653e-03, -4.46416365e-02,  5.41515220e-02, ...,\n",
              "         7.13945272e-03,  4.10880156e-03,  2.36464207e-03],\n",
              "       ...,\n",
              "       [ 3.44433680e-02,  5.06801187e-02, -9.43939036e-03, ...,\n",
              "         1.21178289e-04, -2.39847060e-04,  4.74727057e-04],\n",
              "       [-5.27375548e-02,  5.06801187e-02, -1.15950145e-02, ...,\n",
              "         9.34310152e-04, -1.59551086e-04,  2.72463581e-05],\n",
              "       [-1.88201653e-03, -4.46416365e-02, -2.66843835e-02, ...,\n",
              "         1.78486875e-02,  2.62292701e-03,  3.85448292e-04]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eL_olN1X_-Ja",
        "colab_type": "text"
      },
      "source": [
        "## Regularized Linear Models\n",
        "\n",
        "If we have a lot of features in our training data, we are likely to overfit.\n",
        "\n",
        "However, we don't want to arbitrarily exclude features - after all they might include useful information.\n",
        "\n",
        "One solution is **regularization**: we penalize features in our cost function as well as the fit.\n",
        "\n",
        "For example, in standard linear regression:\n",
        "\n",
        "cost_function = MSE\n",
        "\n",
        "In regularized linear regression:\n",
        "\n",
        "cost_function = MSE + regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uPHNTEA-6Ud",
        "colab_type": "text"
      },
      "source": [
        "Alternatively we can find an approximation of the minimum using gradient descent:\n",
        "\n",
        "![alt text](https://images.deepai.org/glossary-terms/dd6cdd6fcfea4af1a1075aac0b5aa110/sgd.png)\n",
        "\n",
        "This can be a lot quicker when you have large amounts of data (lots of rows), or a lot of features in X (lots of columns).\n",
        "\n",
        "However, you have to decide on a learning rate (don't want this to be too big or too small)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psno_nnI_2HX",
        "colab_type": "text"
      },
      "source": [
        "### Ridge regression\n",
        "\n",
        "In ridge regression, the regularization term is the sum of the coefficients squared.\n",
        "\n",
        "So, our cost function is:\n",
        "\n",
        "$cost function = MSE + \\alpha \\sum{(coefs)^2}$\n",
        "\n",
        "where \n",
        "* $\\alpha$ is a constant set by **you** that determines how strong this regularization penalty is.\n",
        "\n",
        "There's a great video on Youtube that introduces Ridge regression very simply and shows how it's predictions compare to standard regression (~20 mins).\n",
        "\n",
        "> *Note: the narrator recommends a bunch of other videos to watch first, but you can ignore most of these. The Bias-Variance video might be useful if you have never heard of those concepts, but you can also just think of \"Bias\" as underfitting, and \"Variance\" as overfitting.*\n",
        "\n",
        "<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=Q81RR3yKn30\n",
        "\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/Q81RR3yKn30/0.jpg\" \n",
        "alt=\"Elastic Net\" width=\"480\" height=\"360\" border=\"1\" /></a>\n",
        "\n",
        "In Scikit-Learn, we can use the `Ridge` class to do Ridge regression just like we did standard linear regression (more details on Ridge regression with sklearn [in the official guide](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvGP3TvdCtlD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "ridge_reg = Ridge(alpha = 1) # note that we have to set alpha\n",
        "\n",
        "ridge_reg.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBOYU_EzDLpK",
        "colab_type": "text"
      },
      "source": [
        "#### Exercises\n",
        "\n",
        "1. Repeat the steps we carried out for the linear regression above to look at the models fit and accuracy.\n",
        "\n",
        "2. What happens if you set alpha to 0?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pD3abip2DnXX",
        "colab_type": "text"
      },
      "source": [
        "### Lasso regression\n",
        "\n",
        "Like ridge regression, but our regularization term is sum of the absolute values of the coefficients (rather than their squares).\n",
        "\n",
        "$cost function = MSE + \\alpha \\sum{|coefs|}$\n",
        "\n",
        "The same Youtube channel as before has another (even shorter) video on Lasso regression:\n",
        "\n",
        "<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=NGf0voTMlcs\n",
        "\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/NGf0voTMlcs/0.jpg\" \n",
        "alt=\"Elastic Net\" width=\"480\" height=\"360\" border=\"1\" /></a>\n",
        "\n",
        "In Scikit-Learn, we can use the `Lasso` class to do Lasso regression just like we did standard or Ridge regression (more details on Lasso regression with sklearn [in the official guide](https://scikit-learn.org/stable/modules/linear_model.html#lasso))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXsZpxc9Dmua",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "babc21e9-3581-4cac-9ec4-695ba938b97e"
      },
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "lasso_reg = Lasso(alpha = 1) # note that we have to set alpha\n",
        "\n",
        "lasso_reg.fit(X_train, y_train)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Lasso(alpha=1, copy_X=True, fit_intercept=True, max_iter=1000, normalize=False,\n",
              "      positive=False, precompute=False, random_state=None, selection='cyclic',\n",
              "      tol=0.0001, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEE_UTYXFT4j",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise\n",
        "\n",
        "1. Repeat the steps we carried out for the linear regression above to look at the models fit and accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0Tik7nWFVvv",
        "colab_type": "text"
      },
      "source": [
        "### More exercises\n",
        "\n",
        "1. We can combine Ridge and Lasso regression into the same regression model. This is called **ElasticNet** and we can use a sklearn class of the same name: `ElasticNet` (more details in the [documentation](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression)). Try using Elastic Net to create an a different regression model.\n",
        "\n",
        "  You can learn more about Elastic Net in this Youtube video:\n",
        "\n",
        "  <a href=\"http://www.youtube.com/watch?feature=player_embedded&v=1dKRdX9bfIo\n",
        "\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/1dKRdX9bfIo/0.jpg\" \n",
        "alt=\"Elastic Net\" width=\"480\" height=\"360\" border=\"1\" /></a>\n",
        "\n",
        "2. We have to set alpha (the model can't figure it out for us). How do we decide the best value? **Cross-validation!**\n",
        "\n",
        "  i. One way of getting the optimal value of alpha with cross validation is to manually search through values of alpha using the `GridSearchCV` class. Link to documentation \n",
        "\n",
        "  ii. Alternatively there are versions of each regularized linear regression which will do cross validation much more simply:\n",
        "    * [LassoCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html)\n",
        "    * [RidgeCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html)\n",
        "    * [ElasticNetCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNetCV.html)\n",
        "\n",
        "    For example, you can use code like this:\n",
        "\n",
        "    ```python\n",
        "    from sklearn.linear_model import ElasticNetCV\n",
        "\n",
        "    # Values of alpha to try:\n",
        "    alphas = [0.1, 0.5, 1, 2, 4, 8]\n",
        "\n",
        "    elastic_cv = ElasticNetCV(alphas=alphas, cv=5)\n",
        "    elastic_cv.fit(X_train, y_train)\n",
        "    \n",
        "    # Then we use elastic_cv like any of the previous models:\n",
        "    print(elastic_cv.alpha_)\n",
        "    print(elastic_cv.intercept_)\n",
        "    ```"
      ]
    }
  ]
}